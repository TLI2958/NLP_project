{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tl2546/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/scratch/tl2546/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/scratch/tl2546/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/scratch/tl2546/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from dataset_class import *\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "violet = '#702b9d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/scratch/' + os.environ.get(\"USER\", \"\") + '/data/jigsaw_training'\n",
    "val_path =  '/scratch/' + os.environ.get(\"USER\", \"\") + '/data/jigsaw_validation'\n",
    "training_file_path = os.path.join(train_path, 'train.csv')\n",
    "validation_file_path = os.path.join(val_path, 'validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804869</th>\n",
       "      <td>6333967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Maybe the tax on \"things\" would be collected w...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399385</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804870</th>\n",
       "      <td>6333969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>What do you call people who STILL think the di...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399528</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804871</th>\n",
       "      <td>6333982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>thank you ,,,right or wrong,,, i am following ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399457</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804872</th>\n",
       "      <td>6334009</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>Anyone who is quoted as having the following e...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399519</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804873</th>\n",
       "      <td>6334010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Students defined as EBD are legally just as di...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399318</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1804874 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id    target                                       comment_text  \\\n",
       "0          59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1          59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2          59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3          59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4          59856  0.893617               haha you guys are a bunch of losers.   \n",
       "...          ...       ...                                                ...   \n",
       "1804869  6333967  0.000000  Maybe the tax on \"things\" would be collected w...   \n",
       "1804870  6333969  0.000000  What do you call people who STILL think the di...   \n",
       "1804871  6333982  0.000000  thank you ,,,right or wrong,,, i am following ...   \n",
       "1804872  6334009  0.621212  Anyone who is quoted as having the following e...   \n",
       "1804873  6334010  0.000000  Students defined as EBD are legally just as di...   \n",
       "\n",
       "         severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n",
       "0               0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "1               0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "2               0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "3               0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "4               0.021277  0.000000         0.021277  0.872340     0.0    0.0   \n",
       "...                  ...       ...              ...       ...     ...    ...   \n",
       "1804869         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "1804870         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "1804871         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "1804872         0.030303  0.030303         0.045455  0.621212     0.0    NaN   \n",
       "1804873         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "\n",
       "         atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "1            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "2            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "3            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "4            0.0  ...        2006  rejected      0    0    0      1         0   \n",
       "...          ...  ...         ...       ...    ...  ...  ...    ...       ...   \n",
       "1804869      NaN  ...      399385  approved      0    0    0      0         0   \n",
       "1804870      NaN  ...      399528  approved      0    0    0      0         0   \n",
       "1804871      NaN  ...      399457  approved      0    0    0      0         0   \n",
       "1804872      NaN  ...      399519  approved      0    0    0      0         0   \n",
       "1804873      NaN  ...      399318  approved      0    0    0      0         0   \n",
       "\n",
       "         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0                    0.0                         0                         4  \n",
       "1                    0.0                         0                         4  \n",
       "2                    0.0                         0                         4  \n",
       "3                    0.0                         0                         4  \n",
       "4                    0.0                         4                        47  \n",
       "...                  ...                       ...                       ...  \n",
       "1804869              0.0                         0                         4  \n",
       "1804870              0.0                         0                         4  \n",
       "1804871              0.0                         0                         4  \n",
       "1804872              0.0                         0                        66  \n",
       "1804873              0.0                         0                         4  \n",
       "\n",
       "[1804874 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_train = pd.read_csv(f'{training_file_path}')\n",
    "toxicity_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_train['label'] = np.where(toxicity_train.target > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import wordcloud\n",
    "\n",
    "## TODO: complete dataset class just in case\n",
    "class toxic_dataset():\n",
    "    def __init__(self, df, text, toxicity, size = 10, label = None, seed = 1011):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): original dataframe\n",
    "            text (pd.Series): comment text\n",
    "            toxicity (pd.Series): toxic score\n",
    "            size: length of training set\n",
    "            label: toxic (1) or not (0). Keep it in case to calculate accuracy, etc. make sure it is of the same length as text  \n",
    "            seed: random seed. Default to 1011\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.text = text\n",
    "        self.toxicity = toxicity\n",
    "        self.size = size\n",
    "        self.label = label\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.default_rng(seed = self.seed)\n",
    "    def rate_toxicity(self, methods = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            methods: Defaults to None. Use the target \n",
    "            1. linear combination of target, severity toxicity, obscene, \n",
    "            identity_attach, insult, threat. see README.md for details.\n",
    "            2. ?\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "            \n",
    "    def reset_all_indices(self):\n",
    "        \"\"\"\n",
    "        Reset all the indices\n",
    "        \"\"\"\n",
    "        # reset index\n",
    "        self.text.reset_index(drop = True, inplace = True)\n",
    "        self.toxicity.reset_index(drop = True, inplace = True)\n",
    "        self.label.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "    def down_sample(self, threshold, rate = 0.1):\n",
    "        \"\"\"\n",
    "        downsample non-toxic texts\n",
    "        \"\"\"\n",
    "        no_toxic = self.text[self.toxicity <= threshold]\n",
    "        sample_ind = self.rng.integers(0, len(no_toxic), \n",
    "                                        int(len(no_toxic) * rate))\n",
    "        no_toxic = no_toxic.iloc[no_toxic.index[sample_ind]]\n",
    "        toxic = self.text[self.toxicity > threshold]\n",
    "        self.text = pd.concat([no_toxic, toxic], axis = 0)\n",
    "        self.rng.shuffle(self.text)\n",
    "\n",
    "        # update other attributes\n",
    "        self.toxicity = self.toxicity[self.text.index]\n",
    "        self.label = self.label[self.text.index]\n",
    "        self.reset_all_indices()\n",
    "        \n",
    "        \n",
    "    def make_pairs(self, indices = np.zeros((1, 2))):\n",
    "        ## TODO: need to tackle initial condition\n",
    "        # terminate condition\n",
    "        if len(indices) == self.size:\n",
    "            self.indices = indices\n",
    "            self.rearrange()\n",
    "            self.reset_all_indices()\n",
    "            print(self.text, self.toxicity, self.label)\n",
    "            print('paired up ...')\n",
    "            return\n",
    "\n",
    "        add_ind = self.rng.integers(0, len(self.text), \n",
    "                                    size = (self.size - len(indices), 2))\n",
    "        add_ind = np.unique(add_ind, axis = 0)\n",
    "        return self.make_pairs(indices = np.vstack((indices, add_ind))) if len(indices) > 1 \\\n",
    "            else self.make_pairs(indices = add_ind)          \n",
    "    \n",
    "    \n",
    "    def rearrange(self):\n",
    "        indices = self.indices\n",
    "        self.text = pd.concat([self.text.iloc[indices[:,0]].reset_index(drop= True), \n",
    "                                  self.text.iloc[indices[:,1]].reset_index(drop=True)], \n",
    "                              axis = 1)\n",
    "        self.text.columns = ['more_toxic_text', 'less_toxic_text']\n",
    "        \n",
    "        self.toxicity = pd.concat([self.toxicity.loc[indices[:, 0]].reset_index(drop=True),\n",
    "                               self.toxicity.loc[indices[:, 1]].reset_index(drop=True)],\n",
    "                              axis=1)\n",
    "        self.toxicity.columns = ['toxicity_more_toxic', 'toxicity_less_toxic']\n",
    "\n",
    "        self.label = pd.concat([self.label.loc[indices[:, 0]].reset_index(drop=True),\n",
    "                            self.label.loc[indices[:, 1]].reset_index(drop=True)],\n",
    "                           axis=1)\n",
    "        self.label.columns = ['labels_more_toxic', 'labels_less_toxic']\n",
    "\n",
    "\n",
    "\n",
    "        swap = self.toxicity.iloc[:,0] <= self.toxicity.iloc[:,1]\n",
    "        \n",
    "        self.toxicity.loc[swap, ['toxicity_more_toxic', 'toxicity_less_toxic']] = \\\n",
    "            self.toxicity.loc[swap, ['toxicity_less_toxic', 'toxicity_more_toxic']].values\n",
    "        \n",
    "        self.text.loc[swap, ['more_toxic_text', 'less_toxic_text']] = \\\n",
    "            self.text.loc[swap, ['less_toxic_text', 'more_toxic_text']].values\n",
    "        \n",
    "        self.label.loc[swap, ['labels_more_toxic', 'labels_less_toxic']] = \\\n",
    "            self.label.loc[swap, ['labels_less_toxic', 'labels_more_toxic']].values\n",
    "                \n",
    "    def make_dataframe(self, down_sample = False, make_pairs = False):\n",
    "        \"\"\"\n",
    "        Run after self.make_pairs\n",
    "        \"\"\"\n",
    "        if down_sample:\n",
    "            self.down_sample()\n",
    "        if make_pairs:\n",
    "            self.make_pairs()\n",
    "        self.df = pd.concat([self.text, self.toxicity, self.label], axis = 1)\n",
    "        print('made new dataframe ...')\n",
    "        \n",
    "        \n",
    "    def graph(self):\n",
    "        \"\"\"\n",
    "        placeholder for wordcloud\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = toxic_dataset(df = toxicity_train, text = toxicity_train.comment_text, \n",
    "                      toxicity = toxicity_train.target, size = 1000, label = toxicity_train.label, seed = 1011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       more_toxic_text  \\\n",
      "0                           Or read your comments.....   \n",
      "1    perhaps ms. cain can talk with  the other land...   \n",
      "2    The point is: Imago Dei has doctrines that are...   \n",
      "3    No taxes or messing with the Permanent Fund ne...   \n",
      "4           It's hard to ignore a woman on her period.   \n",
      "..                                                 ...   \n",
      "995  It's 2017 and women and girls still think \"we ...   \n",
      "996  Hewing closely to the leftist tendency to prop...   \n",
      "997  What Bavious stated here is so obvious that an...   \n",
      "998               I luv people with 4 names. Sooo cool   \n",
      "999  'despite a change in the indication he is faci...   \n",
      "\n",
      "                                       less_toxic_text  \n",
      "0    The RV sellers have insurance. The insurer has...  \n",
      "1    If there's a list from which a judge can be ch...  \n",
      "2    So many snowflakes upset over a civil question...  \n",
      "3    Virtually all your questions regarding the Fir...  \n",
      "4    Please, tell us how effective the assault weap...  \n",
      "..                                                 ...  \n",
      "995  What evidence do you have for this?\\n\\nI'm goi...  \n",
      "996  Because if we've been doing something horrible...  \n",
      "997  Most Alaskans actually disagree with you, Flus...  \n",
      "998  No I expect the party who perpetuates the war ...  \n",
      "999  Those folks don't need any more a that book le...  \n",
      "\n",
      "[1000 rows x 2 columns]      toxicity_more_toxic  toxicity_less_toxic\n",
      "0               0.000000             0.000000\n",
      "1               0.300000             0.200000\n",
      "2               0.600000             0.000000\n",
      "3               0.000000             0.000000\n",
      "4               0.300000             0.000000\n",
      "..                   ...                  ...\n",
      "995             0.833333             0.166667\n",
      "996             0.166667             0.000000\n",
      "997             0.800000             0.000000\n",
      "998             0.000000             0.000000\n",
      "999             0.000000             0.000000\n",
      "\n",
      "[1000 rows x 2 columns]      labels_more_toxic  labels_less_toxic\n",
      "0                    0                  0\n",
      "1                    0                  0\n",
      "2                    1                  0\n",
      "3                    0                  0\n",
      "4                    0                  0\n",
      "..                 ...                ...\n",
      "995                  1                  0\n",
      "996                  0                  0\n",
      "997                  1                  0\n",
      "998                  0                  0\n",
      "999                  0                  0\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "paired up ...\n"
     ]
    }
   ],
   "source": [
    "toxic.make_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at distribution\n",
    "print(toxicity_train.target.describe()) ## unbalanced sample\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (8,4))\n",
    "sns.set(style=\"whitegrid\") \n",
    "sns.histplot(toxicity_train.target, binwidth = .05, color = violet, ax = ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at distribution\n",
    "print(toxicity_train.severe_toxicity.describe()) ## unbalanced sample\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (8,4))\n",
    "sns.set(style=\"whitegrid\") \n",
    "sns.histplot(toxicity_train.severe_toxicity, binwidth = .05, color = violet, ax = ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jtc = pd.read_csv(f'{os.path.join(train_path, \"train_jtc.csv\")}')\n",
    "train_jtc.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at distribution\n",
    "print(train_jtc.toxic.describe()) ## unbalanced sample\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (8,4))\n",
    "sns.set(style=\"whitegrid\") \n",
    "sns.histplot(train_jtc.toxic, binwidth = .05, color = violet, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_valid = pd.read_csv(f'{validation_file_path}')\n",
    "print(toxicity_valid.nunique())\n",
    "print(toxicity_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess validation dataset \n",
    "texts = set(toxicity_valid.less_toxic.to_list() + toxicity_valid.more_toxic.to_list())\n",
    "text2id = {t:id for id,t in enumerate(texts)}\n",
    "toxicity_valid['less_id'] = toxicity_valid['less_toxic'].map(text2id)\n",
    "toxicity_valid['more_id'] = toxicity_valid['more_toxic'].map(text2id)\n",
    "toxicity_valid_grouped = toxicity_valid.groupby(['less_id', 'more_id']).first().reset_index().drop(['less_id', 'more_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_valid_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
